{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_to_params(individual):\n",
    "    criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, ccp_alpha = individual\n",
    "    \n",
    "    params = {\"criterion\": criterion, \"splitter\": splitter, \"max_depth\": max_depth, \"min_samples_split\": min_samples_split, \"min_samples_leaf\": min_samples_leaf, \"min_weight_fraction_leaf\": min_weight_fraction_leaf, \"max_features\": max_features, \"max_leaf_nodes\": max_leaf_nodes, \"min_impurity_decrease\": min_impurity_decrease, \"ccp_alpha\": ccp_alpha}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(individual, X_train, y_train):\n",
    "    params = individual_to_params(individual)\n",
    "    clf = DecisionTreeClassifier(random_state=42,**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=[0])\n",
    "    # df=df[['SrcWin','sHops','dHops','sTtl','dTtl','SynAck','SrcBytes','DstBytes','SAppBytes',\\\n",
    "    #                    'Dur','TotPkts','TotBytes','TotAppByte','Rate','SrcRate','Label']]\n",
    "    #Le = LabelEncoder()\n",
    "    #df['Label'] = le.fit_transform(df['Label'])\n",
    "    df=df[['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate','Label']]\n",
    "    print(df.shape)\n",
    "    print(\"loading data\")\n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    return X, y,df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../data/'\n",
    "train_file = os.path.join(data_path, 'ISCX_training.csv')\n",
    "test_file = os.path.join(data_path, 'ISCX_Testing.csv')\n",
    "X_train, y_train,train_df = load_and_preprocess(train_file)\n",
    "X_test, y_test,test_df = load_and_preprocess(test_file)\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(X_train_scaled,y_train, test_size=0.01, random_state=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='../optimization/information_feature_selection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_ind=['gini', 'random', 18, 6, 6, 0, None, 90, 0.0, 0.0]\n",
    "# clf = createModel(best_ind, X_train_base, y_train_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=joblib.load(model_path+'best_decision_tree_multiiscx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_scaled)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision: \", precision_score(y_test, predictions))\n",
    "print(\"Recall: \", recall_score(y_test, predictions))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_hat(cal_smx,cal_labels,alpha):\n",
    "    n=cal_smx.shape[0]\n",
    "    cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "    # 2: get adjusted quantile\n",
    "    q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "    qhat = np.quantile(cal_scores, q_level, interpolation='higher')\n",
    "    print(f'q_hat:{qhat}')\n",
    "    return qhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = clf.predict_proba(X_val_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probabilities = clf.predict_proba(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conformity_helper_new import conform_helper\n",
    "conformity_helper=conform_helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iscx_q_hat=get_q_hat(probabilities,y_val_base.astype(int).values,0.05)\n",
    "test_f1, test_precision, test_recall, test_accuracy, test_cm = conformity_helper.calculate_metrics(y_test,predictions)\n",
    "test_TN, test_FP, test_FN, test_TP = test_cm[0][0], test_cm[0][1], test_cm[1][0], test_cm[1][1]\n",
    "test_tb_c, test_tm_c, test_fb_c, test_fm_c, test_tb_Nc, test_tm_Nc, test_fb_Nc, test_fm_Nc, original_test_df, lgb_test_final_prediction_sets = conformity_helper.get_conformity_result(test_TN, test_TP, test_FN, test_FP,test_probabilities ,y_test,predictions,iscx_q_hat)\n",
    "test_coverage_count, test_non_coverage_count, test_coverage_per, test_non_coverage_per = conformity_helper.coverage_modified_value(lgb_test_final_prediction_sets,y_test.astype(int))\n",
    "print(f\"Test| Coverage count| {test_coverage_count} | Non-coverage count| {test_non_coverage_count} | Coverage percentage| {test_coverage_per} | Non-coverage percentage| {test_non_coverage_per}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iscx_q_hat=get_q_hat(probabilities,y_val_base.astype(int).values,0.1)\n",
    "test_f1, test_precision, test_recall, test_accuracy, test_cm = conformity_helper.calculate_metrics(y_test,predictions)\n",
    "test_TN, test_FP, test_FN, test_TP = test_cm[0][0], test_cm[0][1], test_cm[1][0], test_cm[1][1]\n",
    "test_tb_c, test_tm_c, test_fb_c, test_fm_c, test_tb_Nc, test_tm_Nc, test_fb_Nc, test_fm_Nc, original_test_df, lgb_test_final_prediction_sets = conformity_helper.get_conformity_result(test_TN, test_TP, test_FN, test_FP,test_probabilities ,y_test,predictions,iscx_q_hat)\n",
    "test_coverage_count, test_non_coverage_count, test_coverage_per, test_non_coverage_per = conformity_helper.coverage_modified_value(lgb_test_final_prediction_sets,y_test.astype(int))\n",
    "print(f\"Test| Coverage count| {test_coverage_count} | Non-coverage count| {test_non_coverage_count} | Coverage percentage| {test_coverage_per} | Non-coverage percentage| {test_non_coverage_per}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_attack = ['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate']\n",
    "\n",
    "all_adversarial_samples = []\n",
    "\n",
    "for feature in features_to_attack:\n",
    "    filename = \"../output_iscx_new/\" + str(feature) + \"_data.pkl\"\n",
    "    \n",
    "    with open(filename, \"rb\") as file:\n",
    "        loaded_data = pickle.load(file)\n",
    "\n",
    "    adversarial_samples_list = loaded_data['adversarial_samples_list']\n",
    "\n",
    "    # Extend the collector list with non-empty lists\n",
    "    for samples in adversarial_samples_list:\n",
    "        if samples:  # This checks if the list is not empty\n",
    "            all_adversarial_samples.extend(samples)\n",
    "\n",
    "# Convert to numpy array\n",
    "all_adversarial_samples_array = np.array(all_adversarial_samples)\n",
    "\n",
    "print(all_adversarial_samples_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_prediction =clf.predict(all_adversarial_samples_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ones_zero(adversarial_prediction):\n",
    "    count_0s = np.sum(adversarial_prediction  == 0.)\n",
    "    count_1s = np.sum(adversarial_prediction  == 1.)\n",
    "    print(f\"Number of 0s: {count_0s}\")\n",
    "    print(f\"Number of 1s: {count_1s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ones_zero(adversarial_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the adverserial sample should be predicted as malware so we get the samples which is predicted as benign\n",
    "samples_with_prediction_0 = all_adversarial_samples_array[adversarial_prediction == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_with_prediction_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adverserial_conformal(clf,samples_with_prediction_0,iscx_q_hat):\n",
    "    samples_with_prediction_proba=clf.predict_proba(samples_with_prediction_0)\n",
    "    final_prediction_sets=samples_with_prediction_proba >= (1-iscx_q_hat)\n",
    "    conformity=[]\n",
    "    for i in range(0,final_prediction_sets.shape[0]):\n",
    "        if final_prediction_sets[i][0]==final_prediction_sets[i][1]:\n",
    "            conformity.append('No-Conformity')\n",
    "        else:\n",
    "            conformity.append('Conform')\n",
    "    count_no_conformity = conformity.count('No-Conformity')\n",
    "    count_conform = conformity.count('Conform')\n",
    "\n",
    "    print(f\"Number of 'No-Conformity': {count_no_conformity}\")\n",
    "    print(f\"Number of 'Conform': {count_conform}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adverserial_conformal(clf,samples_with_prediction_0,iscx_q_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_adverserial=np.ones(samples_with_prediction_0.shape[0])\n",
    "print(y_adverserial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_y=np.concatenate((y_train_base,y_adverserial),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_x_train = np.concatenate((X_train_base, samples_with_prediction_0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a shuffled index\n",
    "shuffled_index = np.random.permutation(len(combine_y))\n",
    "\n",
    "# Apply the shuffled index to both arrays\n",
    "shuffled_x_train = combine_x_train[shuffled_index]\n",
    "shuffled_y = combine_y[shuffled_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind=['gini', 'random', 18, 6, 6, 0, None, 90, 0.0, 0.0]\n",
    "retrain_clf = createModel(best_ind, shuffled_x_train,shuffled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = retrain_clf.predict(X_test_scaled)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision: \", precision_score(y_test, predictions))\n",
    "print(\"Recall: \", recall_score(y_test, predictions))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_prediction_retrain=retrain_clf.predict(samples_with_prediction_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ones_zero(adversarial_prediction_retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import creator, base, tools, algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants:\n",
    "POPULATION_SIZE = 100\n",
    "P_CROSSOVER = 0.5\n",
    "P_MUTATION = 0.5\n",
    "NUM_GENERATIONS = 100\n",
    "HALL_OF_FAME_SIZE = 10\n",
    "print(\"c\")\n",
    "\n",
    "# Genetic Algorithm constants:\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0,1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# ['entropy', 'random', None, 11, 2, 0, None, 90, 0.0, 0.0]\n",
    "print(\"yo\")\n",
    "CRITERION = [\"gini\", \"entropy\"]\n",
    "SPLITTER = [\"best\", \"random\"]\n",
    "MAX_DEPTH = [None] + list(range(3, 51, 3))\n",
    "MIN_SAMPLES_SPLIT = list(range(2, 21))\n",
    "MIN_SAMPLES_LEAF = list(range(1, 21))\n",
    "MIN_WEIGHT_FRACTION_LEAF = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "MAX_FEATURES = [\"auto\", \"sqrt\", \"log2\", None]\n",
    "MAX_LEAF_NODES = [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "MIN_IMPURITY_DECREASE = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "CCP_ALPHA = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "\n",
    "# Attribute generator \n",
    "toolbox.register(\"attr_criterion\", random.choice, CRITERION)\n",
    "toolbox.register(\"attr_splitter\", random.choice, SPLITTER)\n",
    "toolbox.register(\"attr_max_depth\", random.choice, MAX_DEPTH)\n",
    "toolbox.register(\"attr_min_samples_split\", random.choice, MIN_SAMPLES_SPLIT)\n",
    "toolbox.register(\"attr_min_samples_leaf\", random.choice, MIN_SAMPLES_LEAF)\n",
    "toolbox.register(\"attr_min_weight_fraction_leaf\", random.choice, MIN_WEIGHT_FRACTION_LEAF)\n",
    "toolbox.register(\"attr_max_features\", random.choice, MAX_FEATURES)\n",
    "toolbox.register(\"attr_max_leaf_nodes\", random.choice, MAX_LEAF_NODES)\n",
    "toolbox.register(\"attr_min_impurity_decrease\", random.choice, MIN_IMPURITY_DECREASE)\n",
    "toolbox.register(\"attr_ccp_alpha\", random.choice, CCP_ALPHA)\n",
    "\n",
    "# Structure initializers\n",
    "# Structure initializers\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.attr_criterion, toolbox.attr_splitter, toolbox.attr_max_depth, toolbox.attr_min_samples_split, toolbox.attr_min_samples_leaf, toolbox.attr_min_weight_fraction_leaf, toolbox.attr_max_features, toolbox.attr_max_leaf_nodes, toolbox.attr_min_impurity_decrease, toolbox.attr_ccp_alpha), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=5)\n",
    "# toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "\n",
    "\n",
    "def custom_mutate(individual):\n",
    "    gene = random.randint(0,9) # Select which parameter to mutate\n",
    "    if gene == 0:\n",
    "        individual[0] = toolbox.attr_criterion()\n",
    "    elif gene == 1:\n",
    "        individual[1] = toolbox.attr_splitter()\n",
    "    elif gene == 2:\n",
    "        individual[2] = toolbox.attr_max_depth()\n",
    "    elif gene == 3:\n",
    "        individual[3] = toolbox.attr_min_samples_split()\n",
    "    elif gene == 4:\n",
    "        individual[4] = toolbox.attr_min_samples_leaf()\n",
    "    elif gene == 5:\n",
    "        individual[5] = toolbox.attr_min_weight_fraction_leaf()\n",
    "    elif gene == 6:\n",
    "        individual[6] = toolbox.attr_max_features()\n",
    "    elif gene == 7:\n",
    "        individual[7] = toolbox.attr_max_leaf_nodes()\n",
    "    elif gene == 8:\n",
    "        individual[8] = toolbox.attr_min_impurity_decrease()\n",
    "    elif gene == 9:\n",
    "        individual[9] = toolbox.attr_ccp_alpha()\n",
    "    return individual,\n",
    "\n",
    "toolbox.register(\"mutate\", custom_mutate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalModel(individual, X_train, y_train, X_test, y_test):\n",
    "    clf = createModel(individual, X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    return (f1,accuracy,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makeEvalModel(X_train, y_train, X_test, y_test):\n",
    "    def evalModelWrapper(individual):\n",
    "        return evalModel(individual, X_train, y_train, X_test, y_test)\n",
    "    return evalModelWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a population and evolve it\n",
    "toolbox.register(\"evaluate\", makeEvalModel(shuffled_x_train,shuffled_y, X_test_scaled, y_test))\n",
    "pop = toolbox.population(n=POPULATION_SIZE)\n",
    "hof = tools.HallOfFame(1)\n",
    "#     stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "#     stats.register(\"avg\", np.mean)\n",
    "#     stats.register(\"std\", np.std)\n",
    "#     stats.register(\"min\", np.min)\n",
    "#     stats.register(\"max\", np.max)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values[0]) # for the first objective\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "stats2 = tools.Statistics(lambda ind: ind.fitness.values[1]) # for the second objective\n",
    "stats2.register(\"avg\", np.mean)\n",
    "stats2.register(\"std\", np.std)\n",
    "stats2.register(\"min\", np.min)\n",
    "stats2.register(\"max\", np.max)\n",
    "\n",
    "mstats = tools.MultiStatistics(fitness=stats, fitness2=stats2)\n",
    "\n",
    "\n",
    "pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=P_CROSSOVER, mutpb=P_MUTATION, \n",
    "                                    ngen=NUM_GENERATIONS, stats=mstats, halloffame=hof, verbose=True)\n",
    "\n",
    "# Get the best individual from the Hall of Fame\n",
    "best_ind = hof[0]\n",
    "print(\"Best individual: %s\\nwith fitness: %s\" % (best_ind, best_ind.fitness))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind=['entropy', 'best', 45, 9, 1, 0, 'sqrt', None, 0.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test the best individual on the full data\n",
    "re_optimized_clf = createModel(best_ind,shuffled_x_train,shuffled_y)\n",
    "predictions_reoptimized = re_optimized_clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, predictions_reoptimized)\n",
    "\n",
    "# Save the best model\n",
    "dataset_name='iscx'\n",
    "joblib.dump(clf, 're_optimized_adverserial_dt_model'+dataset_name+'.pkl')\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions_reoptimized))\n",
    "print(\"Precision: \", precision_score(y_test, predictions_reoptimized))\n",
    "print(\"Recall: \", recall_score(y_test, predictions_reoptimized))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions_reoptimized))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions_reoptimized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_predictions = re_optimized_clf.predict(shuffled_x_train)\n",
    "print(\"Accuracy: \", accuracy_score(shuffled_y, train_predictions))\n",
    "print(\"Precision: \", precision_score(shuffled_y, train_predictions))\n",
    "print(\"Recall: \", recall_score(shuffled_y, train_predictions))\n",
    "print(\"F1 score: \", f1_score(shuffled_y, train_predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(shuffled_y, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_prediction_re_optimized=re_optimized_clf.predict(samples_with_prediction_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ones_zero(adversarial_prediction_re_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probabilities_re_optimized = re_optimized_clf.predict_proba(X_val_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_reoptimized_prob = re_optimized_clf.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iscx_q_hat=get_q_hat(val_probabilities_re_optimized,y_val_base.astype(int).values,0.1)\n",
    "test_f1, test_precision, test_recall, test_accuracy, test_cm = conformity_helper.calculate_metrics(y_test,predictions_reoptimized)\n",
    "test_TN, test_FP, test_FN, test_TP = test_cm[0][0], test_cm[0][1], test_cm[1][0], test_cm[1][1]\n",
    "test_tb_c, test_tm_c, test_fb_c, test_fm_c, test_tb_Nc, test_tm_Nc, test_fb_Nc, test_fm_Nc, original_test_df, lgb_test_final_prediction_sets = conformity_helper.get_conformity_result(test_TN, test_TP, test_FN, test_FP,predictions_reoptimized_prob,y_test,predictions_reoptimized,iscx_q_hat)\n",
    "test_coverage_count, test_non_coverage_count, test_coverage_per, test_non_coverage_per = conformity_helper.coverage_modified_value(lgb_test_final_prediction_sets,y_test.astype(int))\n",
    "print(f\"Test| Coverage count| {test_coverage_count} | Non-coverage count| {test_non_coverage_count} | Coverage percentage| {test_coverage_per} | Non-coverage percentage| {test_non_coverage_per}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
