{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=joblib.load('iscx_final_reoptimised_gan_cw_dt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=[0])\n",
    "    # df=df[['SrcWin','sHops','dHops','sTtl','dTtl','SynAck','SrcBytes','DstBytes','SAppBytes',\\\n",
    "    #                    'Dur','TotPkts','TotBytes','TotAppByte','Rate','SrcRate','Label']]\n",
    "    #Le = LabelEncoder()\n",
    "    #df['Label'] = le.fit_transform(df['Label'])\n",
    "    df=df[['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate','Label']]\n",
    "    print(df.shape)\n",
    "    print(\"loading data\")\n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    return X, y,df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../data/'\n",
    "train_file = os.path.join(data_path, 'ISCX_training.csv')\n",
    "test_file = os.path.join(data_path, 'ISCX_Testing.csv')\n",
    "X_train, y_train,train_df = load_and_preprocess(train_file)\n",
    "X_test, y_test,test_df = load_and_preprocess(test_file)\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_base, X_val_base, y_train_base, y_val_base = train_test_split(X_train_scaled,y_train, test_size=0.01, random_state=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test_scaled)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision: \", precision_score(y_test, predictions))\n",
    "print(\"Recall: \", recall_score(y_test, predictions))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_prob = clf.predict_proba(X_val_base)\n",
    "test_prob=clf.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_hat(cal_smx,cal_labels,alpha):\n",
    "    n=cal_smx.shape[0]\n",
    "    cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "    # 2: get adjusted quantile\n",
    "    q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "    qhat = np.quantile(cal_scores, q_level, interpolation='higher')\n",
    "    return qhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_detailed_metrics_optimized(probabilities, true_labels, threshold):\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)\n",
    "    is_correct = predicted_classes == true_labels\n",
    "\n",
    "    final_prediction_sets = probabilities>= (1 - threshold)\n",
    "    decision_vectorized = np.where(np.all(final_prediction_sets, axis=1) | np.all(~final_prediction_sets, axis=1), 'No-Conformity', 'Conform')\n",
    "    decisions = np.array(decision_vectorized) == 'Conform'\n",
    "    # Using boolean masks for calculations\n",
    "    correct_accepted = np.logical_and(decisions, is_correct)\n",
    "    correct_rejected = np.logical_and(~decisions, is_correct)\n",
    "    incorrect_accepted = np.logical_and(decisions, ~is_correct)\n",
    "    incorrect_rejected = np.logical_and(~decisions, ~is_correct)\n",
    "\n",
    "    # Summarize results\n",
    "    metrics = {\n",
    "        'correct_accepted': np.sum(correct_accepted),\n",
    "        'correct_rejected': np.sum(correct_rejected),\n",
    "        'incorrect_accepted': np.sum(incorrect_accepted),\n",
    "        'incorrect_rejected': np.sum(incorrect_rejected),\n",
    "    }\n",
    "\n",
    "    metrics['correct_total_acceptance_percentage'] = (metrics['correct_accepted'] / np.sum(is_correct)) * 100\n",
    "    metrics['incorrect_total_rejection_percentage'] = (metrics['incorrect_rejected'] / np.sum(~is_correct)) * 100\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_detailed_metrics_extended_optimized(probabilities,true_labels, threshold):\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)\n",
    "    is_correct = predicted_classes == true_labels\n",
    "\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)\n",
    "    is_correct = predicted_classes == true_labels\n",
    "\n",
    "    final_prediction_sets = probabilities>= (1 - threshold)\n",
    "    decision_vectorized = np.where(np.all(final_prediction_sets, axis=1) | np.all(~final_prediction_sets, axis=1), 'No-Conformity', 'Conform')\n",
    "    decisions = np.array(decision_vectorized) == 'Conform'\n",
    "    \n",
    "    # Initialize metrics structure\n",
    "    metrics = {\n",
    "        'B': {'correct_accepted': 0, 'correct_rejected': 0, 'incorrect_accepted': 0, 'incorrect_rejected': 0},\n",
    "        'M': {'correct_accepted': 0, 'correct_rejected': 0, 'incorrect_accepted': 0, 'incorrect_rejected': 0},\n",
    "    }\n",
    "    \n",
    "    # Vectorized operations for each label\n",
    "    for label in [0, 1]:  # Assuming binary labels: 0 ('B') and 1 ('M')\n",
    "        label_key = 'B' if label == 0 else 'M'\n",
    "        label_mask = true_labels == label\n",
    "        \n",
    "        # Boolean masks for conditions\n",
    "        correct_mask = is_correct & label_mask\n",
    "        incorrect_mask = ~is_correct & label_mask\n",
    "        accepted_mask = decisions & label_mask\n",
    "        rejected_mask = ~decisions & label_mask\n",
    "        \n",
    "        # Apply masks to compute metrics\n",
    "        metrics[label_key]['correct_accepted'] = np.sum(correct_mask & accepted_mask)\n",
    "        metrics[label_key]['correct_rejected'] = np.sum(correct_mask & rejected_mask)\n",
    "        metrics[label_key]['incorrect_accepted'] = np.sum(incorrect_mask & accepted_mask)\n",
    "        metrics[label_key]['incorrect_rejected'] = np.sum(incorrect_mask & rejected_mask)\n",
    "    \n",
    "    # Aggregate and calculate percentages\n",
    "    metrics = calculate_percentages(metrics)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_percentages(metrics):\n",
    "    for label in ['B', 'M']:\n",
    "        ca = metrics[label]['correct_accepted']\n",
    "        cr = metrics[label]['correct_rejected']\n",
    "        ia = metrics[label]['incorrect_accepted']\n",
    "        ir = metrics[label]['incorrect_rejected']\n",
    "        \n",
    "        total_correct = ca + cr\n",
    "        total_incorrect = ia + ir\n",
    "        \n",
    "        metrics[label].update({\n",
    "            'correctly_predicted_total': total_correct,\n",
    "            'incorrectly_predicted_total': total_incorrect,\n",
    "            'accepted_correct_percentage': 100 * ca / total_correct if total_correct else 0,\n",
    "            'rejected_correct_percentage': 100 * cr / total_correct if total_correct else 0,\n",
    "            'accepted_incorrect_percentage': 100 * ia / total_incorrect if total_incorrect else 0,\n",
    "            'rejected_incorrect_percentage': 100 * ir / total_incorrect if total_incorrect else 0,\n",
    "        })\n",
    "    \n",
    "    # Aggregate 'T' metrics\n",
    "    metrics['T'] = aggregate_totals(metrics)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def aggregate_totals(metrics):\n",
    "    t_metrics = {}\n",
    "    for metric_key in metrics['B']:\n",
    "        t_metrics[metric_key] = metrics['B'][metric_key] + metrics['M'][metric_key]\n",
    "    \n",
    "    # Calculate 'T' percentages based on aggregated totals\n",
    "    t_ca = t_metrics['correct_accepted']\n",
    "    t_cr = t_metrics['correct_rejected']\n",
    "    t_ia = t_metrics['incorrect_accepted']\n",
    "    t_ir = t_metrics['incorrect_rejected']\n",
    "    t_total_correct = t_ca + t_cr\n",
    "    t_total_incorrect = t_ia + t_ir\n",
    "    \n",
    "    t_metrics.update({\n",
    "        'correctly_predicted_total': t_total_correct,\n",
    "        'incorrectly_predicted_total': t_total_incorrect,\n",
    "        'accepted_correct_percentage': 100 * t_ca / t_total_correct if t_total_correct else 0,\n",
    "        'rejected_correct_percentage': 100 * t_cr / t_total_correct if t_total_correct else 0,\n",
    "        'accepted_incorrect_percentage': 100 * t_ia / t_total_incorrect if t_total_incorrect else 0,\n",
    "        'rejected_incorrect_percentage': 100 * t_ir / t_total_incorrect if t_total_incorrect else 0,\n",
    "    })\n",
    "    \n",
    "    return t_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_balanced(probabilities_val, true_labels):\n",
    "    best_threshold = 0.0\n",
    "    best_correct_accept = 0\n",
    "    best_incorrect_reject = 0\n",
    "    best_harmonic_mean = 0  # Best harmonic mean of F1-Accept and F1-Reject\n",
    "    \n",
    "    for threshold in np.linspace(0.5, 0.001, 200):\n",
    "        ucsb_q_hat_lgb_nn=get_q_hat(probabilities_val,y_val_base_ucsb,threshold)\n",
    "        metrics = calculate_detailed_metrics_extended_optimized(probabilities_val, true_labels,ucsb_q_hat_lgb_nn)\n",
    "        correct_accept=metrics['T']['accepted_correct_percentage']\n",
    "        incorrect_reject=metrics['T']['rejected_incorrect_percentage']\n",
    "        \n",
    "        # Calculate the harmonic mean of F1-Accept and F1-Reject\n",
    "        if correct_accept > 0 and incorrect_reject > 0:  # Ensure no division by zero\n",
    "            harmonic_mean = 2 * (correct_accept * incorrect_reject) / (correct_accept + incorrect_reject)\n",
    "        else:\n",
    "            harmonic_mean = 0\n",
    "        \n",
    "        if harmonic_mean > best_harmonic_mean:\n",
    "            best_threshold = threshold\n",
    "            best_correct_accept = correct_accept\n",
    "            best_incorrect_reject = incorrect_reject\n",
    "            best_harmonic_mean = harmonic_mean\n",
    "    \n",
    "    return best_threshold, best_correct_accept, best_incorrect_reject, best_harmonic_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accept_reject_per_optimized(probabilities_test,probabilities_val, true_labels):\n",
    "    thresholds = np.linspace(0.5, 0.001, 300)\n",
    "    accepts, rejects = [], []\n",
    "    for threshold in thresholds:\n",
    "        ucsb_q_hat_lgb_nn=get_q_hat(probabilities_val,y_val_base.astype(int),threshold)\n",
    "        metrics = calculate_detailed_metrics_optimized(probabilities_test, true_labels,ucsb_q_hat_lgb_nn )\n",
    "        accepts.append(metrics['correct_total_acceptance_percentage'])\n",
    "        rejects.append(metrics['incorrect_total_rejection_percentage'])\n",
    "\n",
    "    return accepts, rejects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept, reject = calculate_accept_reject_per_optimized(test_prob,val_prob, y_test.astype(int))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming thresholds are defined as before\n",
    "# Using your provided setup for thresholds\n",
    "thresholds = np.linspace(0.5, 0.001, 300)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define a marker style for each line for better visibility\n",
    "marker_style = dict(linestyle='-', linewidth=3, marker='o', markersize=5, markerfacecolor='white', markevery=10)\n",
    "\n",
    "# Plot for Uncorrected Entropy\n",
    "plt.plot(thresholds, accept, label='$\\mathbf{Accept \\%}$', color='blue', marker='d', markersize=10, markerfacecolor='white', markevery=10)\n",
    "plt.plot(thresholds, reject, label='$\\mathbf{Reject \\%}$', color='blue', linestyle='--', linewidth=3, marker='^', markersize=5, markerfacecolor='white', markevery=10)\n",
    "\n",
    "\n",
    "plt.xticks(fontweight='bold')\n",
    "plt.yticks(fontweight='bold')\n",
    "plt.title('Acceptance and Rejection Percentages using Confromal Prediction', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Threshold', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Percentage', fontsize=16, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('iscx_threshold.png', format='png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_balanced(probabilities_val, true_labels):\n",
    "    best_threshold = 0.0\n",
    "    best_correct_accept = 0\n",
    "    best_incorrect_reject = 0\n",
    "    best_harmonic_mean = 0  # Best harmonic mean of F1-Accept and F1-Reject\n",
    "    \n",
    "    for threshold in np.linspace(0.5, 0.001, 200):\n",
    "        ucsb_q_hat_lgb_nn=get_q_hat(probabilities_val,y_val_base.astype(int),threshold)\n",
    "        metrics = calculate_detailed_metrics_extended_optimized(probabilities_val, true_labels,ucsb_q_hat_lgb_nn)\n",
    "        correct_accept=metrics['T']['accepted_correct_percentage']\n",
    "        incorrect_reject=metrics['T']['rejected_incorrect_percentage']\n",
    "        \n",
    "        # Calculate the harmonic mean of F1-Accept and F1-Reject\n",
    "        if correct_accept > 0 and incorrect_reject > 0:  # Ensure no division by zero\n",
    "            harmonic_mean = 2 * (correct_accept * incorrect_reject) / (correct_accept + incorrect_reject)\n",
    "        else:\n",
    "            harmonic_mean = 0\n",
    "        \n",
    "        if harmonic_mean > best_harmonic_mean:\n",
    "            best_threshold = threshold\n",
    "            best_correct_accept = correct_accept\n",
    "            best_incorrect_reject = incorrect_reject\n",
    "            best_harmonic_mean = harmonic_mean\n",
    "    \n",
    "    return best_threshold, best_correct_accept, best_incorrect_reject, best_harmonic_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'probabilities' and 'true_labels' are defined as before\n",
    "best_threshold, best_correct_accept, best_incorrect_reject, best_harmonic_mean_f1 = find_best_threshold_balanced(X_val_base,y_val_base.astype(int))\n",
    "\n",
    "print(f\"Optimal Threshold: {best_threshold}, Best F1-Accept: {best_correct_accept}, Best F1-Reject: {best_incorrect_reject}, Best Harmonic Mean F1: {best_harmonic_mean_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_hat(cal_smx,cal_labels,alpha):\n",
    "    n=cal_smx.shape[0]\n",
    "    cal_scores = 1-cal_smx[np.arange(n),cal_labels]\n",
    "    # 2: get adjusted quantile\n",
    "    q_level = np.ceil((n+1)*(1-alpha))/n\n",
    "    qhat = np.quantile(cal_scores, q_level, interpolation='higher')\n",
    "    return qhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucsb_q_hat=get_q_hat(test_prob,y_test.values.astype(int),best_threshold)\n",
    "Metrics=calculate_detailed_metrics_extended_optimized(test_prob, y_test.values.astype(int),ucsb_q_hat)\n",
    "Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
