{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_to_params(individual):\n",
    "    criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, ccp_alpha = individual\n",
    "    \n",
    "    params = {\"criterion\": criterion, \"splitter\": splitter, \"max_depth\": max_depth, \"min_samples_split\": min_samples_split, \"min_samples_leaf\": min_samples_leaf, \"min_weight_fraction_leaf\": min_weight_fraction_leaf, \"max_features\": max_features, \"max_leaf_nodes\": max_leaf_nodes, \"min_impurity_decrease\": min_impurity_decrease, \"ccp_alpha\": ccp_alpha}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=[0])\n",
    "    # df=df[['SrcWin','sHops','dHops','sTtl','dTtl','SynAck','SrcBytes','DstBytes','SAppBytes',\\\n",
    "    #                    'Dur','TotPkts','TotBytes','TotAppByte','Rate','SrcRate','Label']]\n",
    "    #Le = LabelEncoder()\n",
    "    #df['Label'] = le.fit_transform(df['Label'])\n",
    "    df=df[['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate','Label']]\n",
    "    print(df.shape)\n",
    "    print(\"loading data\")\n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    return X, y,df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./data/'\n",
    "single_file = os.path.join(data_path, 'isot_botnet.csv')\n",
    "X, y,df = load_and_preprocess(single_file)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  \n",
    "\n",
    "train_indices, test_indices, y_train, y_test = train_test_split(np.arange(len(X)), y, test_size=0.3, random_state=42,shuffle=True)\n",
    "train_df = X.iloc[train_indices]\n",
    "test_df = X.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_df[['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate']])\n",
    "X_test_scaled = scaler.transform(test_df[['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='./optimization/information_feature_selection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf=joblib.load(model_path+'best_rf_exploration_400_indisot_botnet.pkl')\n",
    "predictions = rf_clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "# Print out its metrics\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision: \", precision_score(y_test, predictions))\n",
    "print(\"Recall: \", recall_score(y_test, predictions))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating unscaled max and min values from etf_train_df\n",
    "SrcWin_max = train_df['SrcWin'].max()\n",
    "SrcWin_min = train_df['SrcWin'].min()\n",
    "\n",
    "sHops_max = train_df['sHops'].max()\n",
    "sHops_min = train_df['sHops'].min()\n",
    "\n",
    "sTtl_max = train_df['sTtl'].max()\n",
    "sTtl_min = train_df['sTtl'].min()\n",
    "\n",
    "dTtl_max = train_df['dTtl'].max()\n",
    "dTtl_min = train_df['dTtl'].min()\n",
    "\n",
    "SrcBytes_max = train_df['SrcBytes'].max()\n",
    "SrcBytes_min = train_df['SrcBytes'].min()\n",
    "\n",
    "DstBytes_max = train_df['DstBytes'].max()\n",
    "DstBytes_min = train_df['DstBytes'].min()\n",
    "\n",
    "Dur_max = train_df['Dur'].max()\n",
    "Dur_min = train_df['Dur'].min()\n",
    "\n",
    "TotBytes_max = train_df['TotBytes'].max()\n",
    "TotBytes_min = train_df['TotBytes'].min()\n",
    "\n",
    "Rate_max = train_df['Rate'].max()\n",
    "Rate_min = train_df['Rate'].min()\n",
    "\n",
    "# Calculating scaled min and max values from etf_X_train_norm\n",
    "min_values = X_train_scaled.min(axis=0)\n",
    "max_values = X_train_scaled.max(axis=0)\n",
    "\n",
    "# Assign them to variables named accordingly\n",
    "SrcWin_scaled_min, sHops_scaled_min, sTtl_scaled_min, dTtl_scaled_min, SrcBytes_scaled_min, DstBytes_scaled_min, Dur_scaled_min, TotBytes_scaled_min, Rate_scaled_min = min_values\n",
    "\n",
    "SrcWin_scaled_max, sHops_scaled_max, sTtl_scaled_max, dTtl_scaled_max, SrcBytes_scaled_max, DstBytes_scaled_max, Dur_scaled_max, TotBytes_scaled_max, Rate_scaled_max = max_values\n",
    "\n",
    "# Create a dictionary to store both unscaled and scaled min and max values for each feature\n",
    "feature_bounds = {\n",
    "    'SrcWin': {'min': SrcWin_min, 'max': SrcWin_max, 'scaled_min': SrcWin_scaled_min, 'scaled_max': SrcWin_scaled_max},\n",
    "    'sHops': {'min': sHops_min, 'max': sHops_max, 'scaled_min': sHops_scaled_min, 'scaled_max': sHops_scaled_max},\n",
    "    'sTtl': {'min': sTtl_min, 'max': sTtl_max, 'scaled_min': sTtl_scaled_min, 'scaled_max': sTtl_scaled_max},\n",
    "    'dTtl': {'min': dTtl_min, 'max': dTtl_max, 'scaled_min': dTtl_scaled_min, 'scaled_max': dTtl_scaled_max},\n",
    "    'SrcBytes': {'min': SrcBytes_min, 'max': SrcBytes_max, 'scaled_min': SrcBytes_scaled_min, 'scaled_max': SrcBytes_scaled_max},\n",
    "    'DstBytes': {'min': DstBytes_min, 'max': DstBytes_max, 'scaled_min': DstBytes_scaled_min, 'scaled_max': DstBytes_scaled_max},\n",
    "    'Dur': {'min': Dur_min, 'max': Dur_max, 'scaled_min': Dur_scaled_min, 'scaled_max': Dur_scaled_max},\n",
    "    'TotBytes': {'min': TotBytes_min, 'max': TotBytes_max, 'scaled_min': TotBytes_scaled_min, 'scaled_max': TotBytes_scaled_max},\n",
    "    'Rate': {'min': Rate_min, 'max': Rate_max, 'scaled_min': Rate_scaled_min, 'scaled_max': Rate_scaled_max}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net=load_model(model_path+'optimized_nn_full_training_500isot_botnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = neural_net.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = neural_net.predict(X_test)\n",
    "predictions = [round(x[0]) for x in predictions] \n",
    "# Print out its metrics\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision: \", precision_score(y_test, predictions))\n",
    "print(\"Recall: \", recall_score(y_test, predictions))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_miss_rate_rf=np.round((2611/(2611+31315)*100),2)\n",
    "print(or_miss_rate_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_miss_rate_nn=np.round((4706/(4706+29220)*100),2)\n",
    "print(or_miss_rate_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_pred_index=np.where((np.array(predictions)==1) & (np.array(y_test)==1))[0]\n",
    "X_test_malware=X_test_scaled[malware_pred_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim, feature_count,data_min,data_max):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(128, activation='relu', input_dim=latent_dim),\n",
    "        Dense(feature_count, activation='tanh'),  # Adjusting only targeted features\n",
    "    ])\n",
    "    # Rescale output to match the data range after StandardScaler\n",
    "    scaling_factor = (data_max - data_min) / 2\n",
    "    offset = (data_max + data_min) / 2\n",
    "    model.add(Lambda(lambda x: x * scaling_factor + offset))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_critic(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_min and feature_max are in original space\n",
    "def adjust_feature_dependencies(scaler,perturbed_samples, feature_index,feature_name,feature_min,feature_max):\n",
    "    # Adjust dependencies based on the feature_name\n",
    "    # Add your specific logic here\n",
    "    INITIAL_TTL = 255\n",
    "    perturbed_samples_original = scaler.inverse_transform(perturbed_samples)\n",
    "    # Example for \"Dur\":\n",
    "    perturbed_samples_original[:, feature_index] = np.clip(perturbed_samples_original[:, feature_index], feature_min, feature_max)\n",
    "    \n",
    "\n",
    "    # Adjust dependencies for the modified feature\n",
    "    if feature_name == \"Dur\":\n",
    "        original_duration = perturbed_samples_original[:, 6]\n",
    "        rate_change_factor = original_duration / (perturbed_samples_original[:, 6] + 1e-10)\n",
    "        perturbed_samples_original[:, 8] *= rate_change_factor\n",
    "    elif feature_name == \"SrcBytes\":\n",
    "        perturbed_samples_original[:, 7] = perturbed_samples_original[:, 4] + perturbed_samples_original[:, 5]\n",
    "        # Adjusting Duration to keep Rate constant\n",
    "        original_rate = perturbed_samples_original[:, 8]\n",
    "        perturbed_samples_original[:, 6] = (perturbed_samples_original[:, 4] + perturbed_samples_original[:, 5]) / original_rate\n",
    "    elif feature_name == 'DstBytes':\n",
    "        perturbed_samples_original[:, 7] = perturbed_samples_original[:, 4] + perturbed_samples_original[:, 5]  # TotBytes = SrcBytes + DstBytes\"\n",
    "        # Adjusting Duration to keep Rate constant\n",
    "        original_rate = perturbed_samples_original[:, 8]\n",
    "        perturbed_samples_original[:, 6] = (perturbed_samples_original[:, 4] + perturbed_samples_original[:, 5]) / original_rate\n",
    "    elif feature_name == \"TotBytes\":\n",
    "    # TotBytes is dependent on SrcBytes and DstBytes\n",
    "        perturbed_samples_original[:, 4] = perturbed_samples_original[:, 7] - perturbed_samples_original[:, 5]  # Assuming SrcBytes = TotBytes - DstBytes\n",
    "        perturbed_samples_original[:, 5] = perturbed_samples_original[:, 7] - perturbed_samples_original[:, 4]  # Assuming DstBytes = TotBytes - SrcBytes\n",
    "        \n",
    "        # Adjust Duration to keep Rate constant\n",
    "        original_rate = perturbed_samples_original[:, 8]\n",
    "        perturbed_samples_original[:, 6] = perturbed_samples_original[:, 7] / (original_rate + 1e-10)\n",
    "    elif feature_name == \"sHops\":\n",
    "        perturbed_samples_original[:, 2] = INITIAL_TTL - perturbed_samples_original[:, 1]  # sTtl based on sHops\n",
    "    elif feature_name in [\"sTtl\", \"dTtl\"]:\n",
    "        perturbed_samples_original[:, 1] = INITIAL_TTL - perturbed_samples_original[:, 2]  # sHops based on sTtl\n",
    "    elif feature_name == \"Rate\":\n",
    "        # Adjust Duration based on Rate and TotBytes\n",
    "        perturbed_samples_original[:, 6] = perturbed_samples_original[:, 7] / (perturbed_samples_original[:, 8] + 1e-10)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Rescale to standardized space\n",
    "    perturbed_samples = scaler.transform(perturbed_samples_original)\n",
    "\n",
    "    return perturbed_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(tf.keras.Model):\n",
    "    def __init__(self,real_data,substitute_detector,rf,critic, generator, latent_dim,checkpoints):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.critic = critic\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.substitute_detector=substitute_detector\n",
    "        self.rf=rf\n",
    "        self.checkpoints=checkpoints\n",
    "        self.checkpoint_data= {checkpoint: {'adversarial_samples': [], \n",
    "                                             'nn_miss_rates': 0, \n",
    "                                             'rf_miss_rates': 0, \n",
    "                                             'l2_distances': 0, \n",
    "                                             'unsuccessful_indices': [],\n",
    "                                             'successful_indices': []} for checkpoint in checkpoints}\n",
    "        self.full_dataset=real_data\n",
    "\n",
    "    def compile(self, c_optimizer, g_optimizer, c_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.c_optimizer = c_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.c_loss_fn = c_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def train_step(self,real_data,batch_size,feature_mask,feature_name,feature_index,scaler,scaled_min,scaled_max,ori_min,ori_max,current_iteration):\n",
    "        current_batch_size = tf.shape(real_data)[0]\n",
    "        # Random noise sample\n",
    "        random_latent_vectors = tf.random.normal(shape=(current_batch_size, self.latent_dim))\n",
    "\n",
    "        # Generate fake data using the generator\n",
    "        fake_data = self.generator(random_latent_vectors)\n",
    "        \n",
    "        real_data = tf.cast(real_data, tf.float32)\n",
    "\n",
    "        feature_mask = tf.cast(feature_mask, tf.float32)  # Ensure feature_mask is a float tensor\n",
    "        feature_mask_expanded = tf.expand_dims(feature_mask, 0)  # Expand dimensions to [1, num_features]\n",
    "        feature_mask_expanded = tf.tile(feature_mask_expanded, [current_batch_size, 1])  # Tile to match batch size\n",
    "        # print(feature_mask_expanded)\n",
    "\n",
    "        # print(real_data.shape)\n",
    "        # print(fake_data.shape)\n",
    "        # print('here is the feature mask')\n",
    "        # print(feature_mask_expanded.shape)\n",
    "\n",
    "\n",
    "        modified_data = real_data * (1 - feature_mask_expanded) + fake_data * feature_mask_expanded\n",
    "        # print('here is the modified data')\n",
    "        # print(modified_data)\n",
    "\n",
    "        \n",
    "        # Ensure both real_data and fake_data are of the same type, typically float32\n",
    "        modified_data = tf.cast(modified_data, tf.float32)\n",
    "        feature_mask_single = tf.one_hot(indices=feature_index, depth=tf.shape(modified_data)[1])\n",
    "\n",
    "        # Clip the entire modified_data\n",
    "        clipped_data = tf.clip_by_value(modified_data, scaled_min, scaled_max)\n",
    "\n",
    "        # Apply the mask\n",
    "        modified_data = modified_data * (1 - feature_mask_single) + clipped_data * feature_mask_single\n",
    "\n",
    "        \n",
    "\n",
    "        modified_data=adjust_feature_dependencies(scaler,modified_data,feature_index,feature_name,ori_min,ori_max)\n",
    "        # print('here is the modified data')\n",
    "        # print(modified_data[:,1])\n",
    "        # print('-------')\n",
    "        # print('here is the real data')\n",
    "        # print(real_data[:,1])\n",
    "\n",
    "        # Combine real and fake data\n",
    "        combined_data = tf.concat([real_data, modified_data], axis=0)\n",
    "        combined_labels = tf.concat([tf.ones((batch_size, 1)), -tf.ones((batch_size, 1))], axis=0)\n",
    "      \n",
    "\n",
    "        # Train the critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.critic(combined_data)\n",
    "            c_loss = self.c_loss_fn(combined_labels, predictions)\n",
    "        grads = tape.gradient(c_loss, self.critic.trainable_weights)\n",
    "        self.c_optimizer.apply_gradients(zip(grads, self.critic.trainable_weights))\n",
    "\n",
    "        # Train the generator\n",
    "        random_latent_vectors = tf.random.normal(shape=(current_batch_size, self.latent_dim))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_data = self.generator(random_latent_vectors)\n",
    "\n",
    "\n",
    "            feature_mask = tf.cast(feature_mask, tf.float32)  # Ensure feature_mask is a float tensor\n",
    "            feature_mask_expanded = tf.expand_dims(feature_mask, 0)  # Expand dimensions to [1, num_features]\n",
    "            feature_mask_expanded = tf.tile(feature_mask_expanded, [current_batch_size, 1])  # Tile to match batch size\n",
    "            # print(feature_mask_expanded)\n",
    "\n",
    "            # print(real_data.shape)\n",
    "            # print(fake_data.shape)\n",
    "            # print(feature_mask_expanded.shape)\n",
    "\n",
    "\n",
    "            modified_data = real_data * (1 - feature_mask_expanded) + fake_data * feature_mask_expanded\n",
    "\n",
    "            \n",
    "            # Ensure both real_data and fake_data are of the same type, typically float32\n",
    "            modified_data = tf.cast(modified_data, tf.float32)\n",
    "            feature_mask_single = tf.one_hot(indices=feature_index, depth=tf.shape(modified_data)[1])\n",
    "\n",
    "            # Clip the entire modified_data\n",
    "            clipped_data = tf.clip_by_value(modified_data, scaled_min, scaled_max)\n",
    "\n",
    "            # Apply the mask\n",
    "            modified_data = modified_data * (1 - feature_mask_single) + clipped_data * feature_mask_single\n",
    "\n",
    "            \n",
    "\n",
    "            modified_data=adjust_feature_dependencies(scaler,modified_data,feature_index,feature_name,ori_min,ori_max)\n",
    "\n",
    "\n",
    "            fake_pred = self.critic(fake_data)\n",
    "            substitute_pred = self.substitute_detector(modified_data)\n",
    "            \n",
    "            if current_iteration in self.checkpoints:\n",
    "                successful_idx=np.where(substitute_pred < 0.5)[0]\n",
    "                \n",
    "                self.checkpoint_data[current_iteration]['adversarial_samples'].extend(modified_data[successful_idx])\n",
    "                \n",
    "            g_loss = self.g_loss_fn(fake_pred,substitute_pred)  # Pass only the fake predictions to the generator loss function\n",
    "            # print(g_loss)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        \n",
    "\n",
    "    \n",
    "        # if current_iteration in self.checkpoints:\n",
    "            # nn_miss_rate, rf_miss_rate, l2_distance, unsuccessful_indices = self.evaluate_checkpoint(modified_data, real_data,self.substitute_detector,self.rf)\n",
    "            \n",
    "            # self.checkpoint_data[current_iteration]['adversarial_samples'] = modified_data\n",
    "            # self.checkpoint_data[current_iteration]['nn_miss_rates'] = nn_miss_rate\n",
    "            # self.checkpoint_data[current_iteration]['rf_miss_rates'] = rf_miss_rate\n",
    "            # self.checkpoint_data[current_iteration]['l2_distances'] = l2_distance\n",
    "            # self.checkpoint_data[current_iteration]['unsuccessful_indices'] = unsuccessful_indices\n",
    "\n",
    "\n",
    "        return {\"c_loss\": c_loss, \"g_loss\": g_loss}\n",
    "\n",
    "    def post_batch_evaluation(self, current_iteration):\n",
    "        \"\"\"Evaluate aggregated data after all batches are processed for a checkpoint.\"\"\"\n",
    "        if current_iteration in self.checkpoints:\n",
    "            aggregated_samples = np.array(self.checkpoint_data[current_iteration]['adversarial_samples'])\n",
    "            print(f'Evaluation iteration{current_iteration}')\n",
    "            nn_miss_rate, rf_miss_rate, l2_distance,sucessful_index, unsuccessful_indices = self.evaluate_checkpoint(aggregated_samples,self.full_dataset, self.substitute_detector, self.rf)\n",
    "            self.checkpoint_data[current_iteration]['successful_indices']=sucessful_index\n",
    "            self.checkpoint_data[current_iteration]['nn_miss_rates'] = nn_miss_rate\n",
    "            self.checkpoint_data[current_iteration]['rf_miss_rates'] = rf_miss_rate\n",
    "            self.checkpoint_data[current_iteration]['l2_distances'] = l2_distance\n",
    "            self.checkpoint_data[current_iteration]['unsuccessful_indices'] = unsuccessful_indices\n",
    "    \n",
    "    def evaluate_checkpoint(self, successful_adversarial_samples, real_data, neural_net, rf_model):\n",
    "        \n",
    "        if len(successful_adversarial_samples)>0:\n",
    "            substitute_pred = self.substitute_detector(successful_adversarial_samples)\n",
    "            successful_idx=np.where(substitute_pred < 0.5)[0]\n",
    "                    \n",
    "            # Calculate NN misclassification rate\n",
    "            nn_misclassification_rate = np.round(((len(successful_adversarial_samples) +4706) /(4706+29220)*100), 2)\n",
    "            print(nn_misclassification_rate)\n",
    "\n",
    "            # Calculate L2 distance for successful adversarial samples\n",
    "            real_samples_for_successful = real_data[successful_idx]\n",
    "            l2_distance = np.linalg.norm(real_samples_for_successful - successful_adversarial_samples, axis=1).mean()\n",
    "            print(f'L2 dist{l2_distance}')\n",
    "\n",
    "            # Determine unsuccessful indices\n",
    "            unsuccessful_indices = np.setdiff1d(np.arange(len(successful_adversarial_samples)), successful_idx)\n",
    "            print(f'NN Misclassication Rate{nn_misclassification_rate}')\n",
    "            \n",
    "        else:\n",
    "            nn_misclassification_rate = np.round(((0 +4706) /(4706+29220)*100), 2)\n",
    "            print(nn_misclassification_rate)\n",
    "            real_samples_for_successful = 0\n",
    "            l2_distance = 0\n",
    "            print(f'L2 dist{l2_distance}')\n",
    "            # Determine unsuccessful indices\n",
    "            unsuccessful_indices = 0\n",
    "            successful_idx=None\n",
    "        \n",
    "\n",
    "        # Calculate RF misclassification rate\n",
    "        if len(successful_adversarial_samples)>0:\n",
    "            rf_predictions = rf_model.predict(successful_adversarial_samples)\n",
    "            rf_malware_count = np.count_nonzero(rf_predictions == 0)\n",
    "            rf_misclassification_rate = np.round(((rf_malware_count+2611) /(2611+31315)*100),2)\n",
    "        else:\n",
    "            rf_malware_count = 0\n",
    "            rf_misclassification_rate = np.round(((rf_malware_count+2611) /(2611+31315)*100),2)\n",
    "        print(f'RF Misclassificaiotn Rate{rf_misclassification_rate}')\n",
    "\n",
    "        \n",
    "        return nn_misclassification_rate, rf_misclassification_rate, l2_distance,successful_idx, unsuccessful_indices\n",
    "    def save_checkpoint_data(self,feature):\n",
    "        # print(self.checkpoint_data)\n",
    "\n",
    "        file_path = f\"D:\\\\Network-Revisit\\\\output_gan_isot\\\\{feature}_data.pkl\"\n",
    "\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            pickle.dump(self.checkpoint_data, file)\n",
    "\n",
    "        checkpoints = sorted(self.checkpoint_data.keys())\n",
    "        nn_miss_rates = [self.checkpoint_data[checkpoint]['nn_miss_rates'] for checkpoint in checkpoints]\n",
    "        rf_miss_rates = [self.checkpoint_data[checkpoint]['rf_miss_rates'] for checkpoint in checkpoints]\n",
    "        l2_distances = [self.checkpoint_data[checkpoint]['l2_distances'] for checkpoint in checkpoints]\n",
    "\n",
    "\n",
    "        # Plot for Misclassification Rates\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(checkpoints, nn_miss_rates, '-o', label='NN Miss Rate')\n",
    "        plt.plot(checkpoints, rf_miss_rates, '-s', color='red', label='RF Miss Rate')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('Rate')\n",
    "        plt.title('NN-MR(%) & RF-MR(%) vs. iteration')\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"D:\\\\Network-Revisit\\\\output_gan_isot\\\\{feature}_NN_RF_MR_vs_iteration.png\", dpi=100)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot for L2 Distances\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        ax1 = plt.gca()\n",
    "        ax1.plot(checkpoints, l2_distances, '-o', color='tab:red', label='L2 Distance')\n",
    "        ax1.set_xlabel('Iterations')\n",
    "        ax1.set_ylabel('L2 Distance', color='tab:red')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "        ax1.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "        # Optional: if you want to plot Misclassification Rate on the secondary axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(checkpoints, nn_miss_rates, '-s', color='tab:blue', label='NN Miss Rate')\n",
    "        ax2.set_ylabel('Misclassification Rate', color='tab:blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "        plt.title('Trade-off: Perturbation Magnitude vs Misclassification Rate')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"D:\\\\Network-Revisit\\\\output_gan_isot\\\\{feature}_L2_vs_Misclassification_Rate.png\", dpi=100)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        return self.checkpoint_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_loss(real_pred, fake_pred):\n",
    "    return tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred)\n",
    "\n",
    "def generator_loss(fake_pred, substitute_detector_pred):\n",
    "    # Standard GAN generator loss\n",
    "    gan_loss = -tf.reduce_mean(fake_pred)\n",
    "\n",
    "    # Loss to encourage misclassification by the substitute detector\n",
    "    # Directly minimize the probability of the malware class\n",
    "    substitute_loss = tf.reduce_mean(substitute_detector_pred)\n",
    "\n",
    "    return gan_loss + substitute_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2000  # Adjust as needed\n",
    "epochs = 2001  # Number of epochs\n",
    "\n",
    "# batch_size = 2000  # Adjust as needed\n",
    "# epochs = 10  # Number of epochs\n",
    "\n",
    "\n",
    "# feature_list=['TotBytes', 'Rate']\n",
    "feature_list=['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate']\n",
    "# epochs=10\n",
    "# Assuming X_test_malware is preprocessed and ready for training\n",
    "for feature_name in feature_list:\n",
    "\n",
    "    checkpoints = [5,100,750,1000,2000]\n",
    "    # checkpoints=[1,5,8]\n",
    "    latent_dim = 100  # Adjust as needed\n",
    "    feature_count = 9  # Assuming 10 features\n",
    "    min_value = np.min(X_test_malware)\n",
    "    max_value = np.max(X_test_malware)\n",
    "    # Instantiate models\n",
    "    generator = build_generator(latent_dim, feature_count,min_value,max_value)\n",
    "    critic = build_critic((feature_count,))\n",
    "\n",
    "    # Instantiate WGAN model\n",
    "    wgan = WGAN(X_test_malware,neural_net,rf_clf,critic=critic, generator=generator, latent_dim=latent_dim,checkpoints=checkpoints)\n",
    "\n",
    "    # Compile WGAN\n",
    "    wgan.compile(\n",
    "        c_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        c_loss_fn=critic_loss,\n",
    "        g_loss_fn=generator_loss\n",
    "    )\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_test_malware).batch(batch_size)\n",
    "    # dataset_rate= tf.data.Dataset.from_tensor_slices(rate_data).batch(batch_size)\n",
    "    # Zipping the two datasets together\n",
    "    # zipped_dataset = tf.data.Dataset.zip((dataset, dataset_rate))\n",
    "\n",
    "    feature_index = ['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate'].index(feature_name)\n",
    "    feature_mask = tf.constant([1 if i == feature_index else 0 for i in range(len(['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate']))], dtype=tf.float32)\n",
    "    current_iteration=0\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        if (epoch+1)%200==0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_data  in dataset:\n",
    "            # print('here is the batch data')\n",
    "            # print(batch_data)\n",
    "            wgan.train_step(batch_data,batch_size,feature_mask,feature_name,feature_index,scaler,feature_bounds[feature_name]['scaled_min'], \\\n",
    "                            feature_bounds[feature_name]['scaled_max'],feature_bounds[feature_name]['min'],feature_bounds[feature_name]['max'],current_iteration)\n",
    "        if current_iteration in checkpoints:\n",
    "            wgan.post_batch_evaluation(current_iteration)\n",
    "        current_iteration+=1\n",
    "    wgan.save_checkpoint_data(feature_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
