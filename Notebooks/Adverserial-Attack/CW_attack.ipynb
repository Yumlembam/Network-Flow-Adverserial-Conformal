{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_to_params(individual):\n",
    "    criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, ccp_alpha = individual\n",
    "    \n",
    "    params = {\"criterion\": criterion, \"splitter\": splitter, \"max_depth\": max_depth, \"min_samples_split\": min_samples_split, \"min_samples_leaf\": min_samples_leaf, \"min_weight_fraction_leaf\": min_weight_fraction_leaf, \"max_features\": max_features, \"max_leaf_nodes\": max_leaf_nodes, \"min_impurity_decrease\": min_impurity_decrease, \"ccp_alpha\": ccp_alpha}\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(individual, X_train, y_train):\n",
    "    params = individual_to_params(individual)\n",
    "    clf = DecisionTreeClassifier(random_state=42,**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(filepath):\n",
    "    df = pd.read_csv(filepath, index_col=[0])\n",
    "    # df=df[['SrcWin','sHops','dHops','sTtl','dTtl','SynAck','SrcBytes','DstBytes','SAppBytes',\\\n",
    "    #                    'Dur','TotPkts','TotBytes','TotAppByte','Rate','SrcRate','Label']]\n",
    "    #Le = LabelEncoder()\n",
    "    #df['Label'] = le.fit_transform(df['Label'])\n",
    "    df=df[['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate','Label']]\n",
    "    print(df.shape)\n",
    "    print(\"loading data\")\n",
    "    X = df.iloc[:,:-1]\n",
    "    y = df.iloc[:,-1]\n",
    "    return X, y,df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path='../data/'\n",
    "train_file = os.path.join(data_path, 'ISCX_training.csv')\n",
    "test_file = os.path.join(data_path, 'ISCX_Testing.csv')\n",
    "X_train, y_train,train_df = load_and_preprocess(train_file)\n",
    "X_test, y_test,test_df = load_and_preprocess(test_file)\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Decision tree default\n",
    "# dt_default_clf = DecisionTreeClassifier(random_state=42)\n",
    "# dt_default_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions =dt_default_clf.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "# print(\"Precision: \", precision_score(y_test, predictions))\n",
    "# print(\"Recall: \", recall_score(y_test, predictions))\n",
    "# print(\"F1 score: \", f1_score(y_test, predictions))\n",
    "# print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='../optimization/information_feature_selection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf=joblib.load(model_path+'best_decision_tree_multiiscx.pkl')\n",
    "predictions = dt_clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "# Print out its metrics\n",
    "predictions = dt_clf.predict(X_test_scaled)\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision: \", precision_score(y_test, predictions))\n",
    "print(\"Recall: \", recall_score(y_test, predictions))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the max and min values for each column\n",
    "SrcWin_max = train_df['SrcWin'].max()\n",
    "SrcWin_min = train_df['SrcWin'].min()\n",
    "\n",
    "sHops_max = train_df['sHops'].max()\n",
    "sHops_min = train_df['sHops'].min()\n",
    "\n",
    "sTtl_max = train_df['sTtl'].max()\n",
    "sTtl_min = train_df['sTtl'].min()\n",
    "\n",
    "dTtl_max = train_df['dTtl'].max()\n",
    "dTtl_min = train_df['dTtl'].min()\n",
    "\n",
    "SrcBytes_max = train_df['SrcBytes'].max()\n",
    "SrcBytes_min = train_df['SrcBytes'].min()\n",
    "\n",
    "DstBytes_max = train_df['DstBytes'].max()\n",
    "DstBytes_min = train_df['DstBytes'].min()\n",
    "\n",
    "Dur_max = train_df['Dur'].max()\n",
    "Dur_min = train_df['Dur'].min()\n",
    "\n",
    "TotBytes_max = train_df['TotBytes'].max()\n",
    "TotBytes_min = train_df['TotBytes'].min()\n",
    "\n",
    "Rate_max = train_df['Rate'].max()\n",
    "Rate_min = train_df['Rate'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = X_train_scaled.min(axis=0)\n",
    "max_values = X_train_scaled.max(axis=0)\n",
    "\n",
    "# Assign them to variables named accordingly\n",
    "SrcWin_scaled_min, sHops_scaled_min, sTtl_scaled_min, dTtl_scaled_min, SrcBytes_scaled_min, DstBytes_scaled_min, Dur_scaled_min, TotBytes_scaled_min, Rate_scaled_min = min_values\n",
    "\n",
    "SrcWin_scaled_max, sHops_scaled_max, sTtl_scaled_max, dTtl_scaled_max, SrcBytes_scaled_max, DstBytes_scaled_max, Dur_scaled_max, TotBytes_scaled_max, Rate_scaled_max = max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store min and max values for each feature\n",
    "feature_bounds = {\n",
    "    'SrcWin': {'min': SrcWin_min, 'max': SrcWin_max, 'scaled_min': SrcWin_scaled_min, 'scaled_max': SrcWin_scaled_max},\n",
    "    'sHops': {'min': sHops_min, 'max': sHops_max, 'scaled_min': sHops_scaled_min, 'scaled_max': sHops_scaled_max},\n",
    "    'sTtl': {'min': sTtl_min, 'max': sTtl_max, 'scaled_min': sTtl_scaled_min, 'scaled_max': sTtl_scaled_max},\n",
    "    'dTtl': {'min': dTtl_min, 'max': dTtl_max, 'scaled_min': dTtl_scaled_min, 'scaled_max': dTtl_scaled_max},\n",
    "    'SrcBytes': {'min': SrcBytes_min, 'max': SrcBytes_max, 'scaled_min': SrcBytes_scaled_min, 'scaled_max': SrcBytes_scaled_max},\n",
    "    'DstBytes': {'min': DstBytes_min, 'max': DstBytes_max, 'scaled_min': DstBytes_scaled_min, 'scaled_max': DstBytes_scaled_max},\n",
    "    'Dur': {'min': Dur_min, 'max': Dur_max, 'scaled_min': Dur_scaled_min, 'scaled_max': Dur_scaled_max},\n",
    "    'TotBytes': {'min': TotBytes_min, 'max': TotBytes_max, 'scaled_min': TotBytes_scaled_min, 'scaled_max': TotBytes_scaled_max},\n",
    "    'Rate': {'min': Rate_min, 'max': Rate_max, 'scaled_min': Rate_scaled_min, 'scaled_max': Rate_scaled_max},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net=load_model(model_path+'optimized_nn_full_training_500iscx.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = neural_net.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = neural_net.predict(X_test)\n",
    "predictions = [round(x[0]) for x in predictions] \n",
    "# Print out its metrics\n",
    "print(\"Accuracy: \", accuracy_score(y_test, predictions))\n",
    "print(\"Precision: \", precision_score(y_test, predictions))\n",
    "print(\"Recall: \", recall_score(y_test, predictions))\n",
    "print(\"F1 score: \", f1_score(y_test, predictions))\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_miss_rate_nn=np.round((15430/(15430+147848))*100,2)\n",
    "print(or_miss_rate_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_miss_rate_nn=dt_miss_rate=np.round(((5586) /(5586+157692)*100),2)\n",
    "print(or_miss_rate_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_pred_index=np.where((np.array(predictions)==1) & (np.array(y_test)==1))[0]\n",
    "X_test_malware=X_test_scaled[malware_pred_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x_prime, model, target_class, kappa=0):\n",
    "    Z = model(x_prime)\n",
    "    if target_class == 0:\n",
    "        Z_target = 1. - Z[:, 0]\n",
    "        Z_other = Z[:, 0]\n",
    "    else:\n",
    "        Z_target = Z[:, 0]\n",
    "        Z_other = 1. - Z_target\n",
    "    return tf.maximum(Z_other - Z_target, -kappa)\n",
    "\n",
    "# def generate_significant_noise(shape, feature_mask, min_val=-0.07741, max_val=23):\n",
    "#     noise = tf.random.uniform(shape, 0, 1)  # noise values between 0 and 1\n",
    "#     scaled_noise = noise * (max_val - min_val) + min_val  # scale and shift to desired range\n",
    "#     return scaled_noise * feature_mask\n",
    "\n",
    "def cw_loss(x, x_prime, model, target_class, c):\n",
    "    l2_dist = tf.reduce_sum(tf.square(x - x_prime))\n",
    "    return l2_dist + c * f(x_prime, model, target_class)\n",
    "\n",
    "def generate_relative_noise(x, feature_mask, magnitude=0.1):\n",
    "    # Generate random values between -1 and 1\n",
    "    noise = tf.random.uniform(x.shape, -1, 1)  # noise values between -1 and 1\n",
    "    # Scale noise based on magnitude and original value\n",
    "    relative_noise = noise * x * magnitude\n",
    "    return relative_noise * feature_mask\n",
    "\n",
    "def generate_cw_adversary(model, x, target_class, feature_mask,c, epsilon=0.001, iterations=5, clip_min=-2, clip_max=2,):\n",
    "\n",
    "    noise_shape = list(x.shape)\n",
    "    noise_shape[-1] = 1  # Adjust to match the feature dimensionality\n",
    "    # noise = generate_significant_noise(noise_shape, feature_mask,min_val=SrcBytes_scaled_min,max_val=SrcBytes_scaled_max)\n",
    "    noise = generate_relative_noise(x, feature_mask, magnitude=0.1)\n",
    "    x_prime_init = x + noise\n",
    "    x_prime = tf.Variable(x_prime_init, dtype=tf.float32, trainable=True)\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=epsilon)\n",
    "    for iteration in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # loss = cw_loss_dynamic(x, x_prime, model, target_class, c_f, c_l2)\n",
    "            loss = cw_loss(x, x_prime, model, target_class,c)\n",
    "        grads = tape.gradient(loss, x_prime)\n",
    "        # Mask the gradients to only update the desired feature\n",
    "        masked_grads = grads * feature_mask\n",
    "        # print(\"this is masked grads\")\n",
    "        # print(masked_grads)\n",
    "\n",
    "        optimizer.apply_gradients([(masked_grads, x_prime)])\n",
    "\n",
    "        # Clip x_prime to ensure values stay within reasonable bounds\n",
    "        clipped_values = tf.clip_by_value(x_prime[:, feature_mask.numpy().nonzero()[0][0]], clip_min, clip_max)\n",
    "        x_prime_array = x_prime.numpy()\n",
    "        x_prime_array[:, feature_mask.numpy().nonzero()[0][0]] = clipped_values\n",
    "        x_prime.assign(x_prime_array)\n",
    "        \n",
    "    return x_prime.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Batch function to apply C&W method\n",
    "def cw_batch(model, scaler, input_samples, target_class=0, feature_name=\"Dur\", feature_min=0.0, feature_max=126.0,it_value=0,c=0.001):\n",
    "    INITIAL_TTL = 255\n",
    "    with tf.device('/GPU:0'):\n",
    "        target_labels = tf.zeros_like(input_samples[:, 0])\n",
    "        original = scaler.inverse_transform(input_samples.numpy())\n",
    "        feature_index = ['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate'].index(feature_name)\n",
    "        # print(f\"Before {feature_name}\")\n",
    "        # print(original[:, feature_index])\n",
    "\n",
    "        # Create the mask for the specific feature\n",
    "        feature_mask = tf.constant([1 if i == feature_index else 0 for i in range(len(['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate']))], dtype=tf.float32)\n",
    "        \n",
    "        perturbed_samples = generate_cw_adversary(model, input_samples, target_class, feature_mask,c,epsilon=0.001,iterations=it_value,clip_min=feature_bounds[feature_name]['scaled_min'],clip_max=feature_bounds[feature_name]['scaled_max'])\n",
    "\n",
    "        perturbed_samples_original = scaler.inverse_transform(perturbed_samples)\n",
    "\n",
    "        # Clip in original space\n",
    "        perturbed_samples_original[:, feature_index] = np.clip(perturbed_samples_original[:, feature_index], feature_min, feature_max)\n",
    "        # print(f\"After Perturbation {feature_name}\")\n",
    "        # print(perturbed_samples_original[:, feature_index])\n",
    "\n",
    "        # Adjust dependencies for the modified feature\n",
    "        if feature_name == \"Dur\":\n",
    "            original_duration = perturbed_samples_original[:, 6]\n",
    "            rate_change_factor = original_duration / (perturbed_samples_original[:, 6] + 1e-10)\n",
    "            perturbed_samples_original[:, 8] *= rate_change_factor\n",
    "        elif feature_name == \"SrcBytes\":\n",
    "            perturbed_samples_original[:, 7] = perturbed_samples_original[:, 4] + perturbed_samples_original[:, 5]\n",
    "            # Adjusting Duration to keep Rate constant\n",
    "            original_rate = perturbed_samples_original[:, 8]\n",
    "            perturbed_samples_original[:, 6] = (perturbed_samples_original[:, 4] + perturbed_samples_original[:, 5]) / original_rate\n",
    "        elif feature_name == 'DstBytes':\n",
    "            perturbed_samples_original[:, 7] = perturbed_samples_original[:, 4] + perturbed_samples_original[:, 5]  # TotBytes = SrcBytes + DstBytes\"\n",
    "            # Adjusting Duration to keep Rate constant\n",
    "            original_rate = perturbed_samples_original[:, 8]\n",
    "            perturbed_samples_original[:, 6] = (perturbed_samples_original[:, 4] + perturbed_samples_original[:, 5]) / original_rate\n",
    "        elif feature_name == \"TotBytes\":\n",
    "        # TotBytes is dependent on SrcBytes and DstBytes\n",
    "            perturbed_samples_original[:, 4] = perturbed_samples_original[:, 7] - perturbed_samples_original[:, 5]  # Assuming SrcBytes = TotBytes - DstBytes\n",
    "            perturbed_samples_original[:, 5] = perturbed_samples_original[:, 7] - perturbed_samples_original[:, 4]  # Assuming DstBytes = TotBytes - SrcBytes\n",
    "            \n",
    "            # Adjust Duration to keep Rate constant\n",
    "            original_rate = perturbed_samples_original[:, 8]\n",
    "            perturbed_samples_original[:, 6] = perturbed_samples_original[:, 7] / (original_rate + 1e-10)\n",
    "        elif feature_name == \"sHops\":\n",
    "            perturbed_samples_original[:, 2] = INITIAL_TTL - perturbed_samples_original[:, 1]  # sTtl based on sHops\n",
    "        elif feature_name in [\"sTtl\", \"dTtl\"]:\n",
    "            perturbed_samples_original[:, 1] = INITIAL_TTL - perturbed_samples_original[:, 2]  # sHops based on sTtl\n",
    "        elif feature_name == \"Rate\":\n",
    "            # Adjust Duration based on Rate and TotBytes\n",
    "            perturbed_samples_original[:, 6] = perturbed_samples_original[:, 7] / (perturbed_samples_original[:, 8] + 1e-10)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Rescale to standardized space\n",
    "        perturbed_samples = scaler.transform(perturbed_samples_original)\n",
    "\n",
    "        return perturbed_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_malware_tensor = tf.convert_to_tensor(X_test_malware, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Adversarial sample generation with batching\n",
    "BATCH_SIZE = 10000  # Modify based on your hardware\n",
    "total_batches = int(np.ceil(X_test_malware_tensor.shape[0] / BATCH_SIZE))\n",
    "\n",
    "def generate_adversarial_for_c(c_value,iter,f_name):\n",
    "\n",
    "    \n",
    "    adversarial_samples = []\n",
    "    unsuccessful_samples = []  # List to store unsuccessful samples\n",
    "    unsuccessful_samples_indices = []\n",
    "\n",
    "    for batch in range(total_batches):\n",
    "        start_idx = batch * BATCH_SIZE\n",
    "        end_idx = min((batch+1) * BATCH_SIZE, X_test_malware_tensor.shape[0])\n",
    "        \n",
    "        batch_samples = X_test_malware_tensor[start_idx:end_idx]\n",
    "\n",
    "        # Use C&W method to generate adversarial samples. We target the benign class (class 0)\n",
    "        batch_samples = cw_batch(neural_net, scaler, batch_samples, target_class=0,feature_name=f_name,feature_min=feature_bounds[f_name]['min'],feature_max=feature_bounds[f_name]['max'],it_value=iter, c=c_value)  # Targeting benign class\n",
    "        \n",
    "        with tf.device('/GPU:0'):\n",
    "            surrogate_preds = neural_net.predict(batch_samples)\n",
    "            \n",
    "        # We're looking for malware samples that are now predicted as benign\n",
    "        successful_idx = np.where(surrogate_preds < 0.5)[0]\n",
    "        adversarial_samples.extend(batch_samples[successful_idx])\n",
    "\n",
    "        remaining_indices = np.setdiff1d(np.arange(batch_samples.shape[0]), successful_idx)\n",
    "        unsuccessful_samples.extend(batch_samples[remaining_indices]) \n",
    "\n",
    "        unsuccessful_samples_indices.extend([start_idx + idx for idx in remaining_indices])\n",
    "\n",
    "        print(f\"Processed batch {batch+1}/{total_batches}\")\n",
    "    print(f\"Number of unsuccessful samples: {len(unsuccessful_samples_indices)}\")\n",
    "    # print(f\"Indices of unsuccessful samples: {unsuccessful_samples_indices}\")\n",
    "    if len(adversarial_samples)>0:\n",
    "        adversarial_samples_arr=np.array(adversarial_samples)\n",
    "        predictions = dt_clf.predict(adversarial_samples_arr)\n",
    "        print(predictions)\n",
    "        # Count the number of 1s in the array\n",
    "        dt_malware_count = np.count_nonzero(predictions == 0)\n",
    "        dt_miss_rate=np.round(((dt_malware_count+5586) /(5586+157692)*100),2)\n",
    "    else:\n",
    "        dt_malware_count=0\n",
    "        dt_miss_rate=np.round(((dt_malware_count+5586) /(5586+157692)*100),2)\n",
    "\n",
    "    \n",
    "    return np.round(((len(adversarial_samples)+15430) /(15430+147848)*100),2),dt_miss_rate ,unsuccessful_samples_indices,adversarial_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of iteration values\n",
    "features_to_attack = ['SrcWin', 'sHops', 'sTtl', 'dTtl', 'SrcBytes', 'DstBytes', 'Dur', 'TotBytes', 'Rate']\n",
    "iteration_values = [5,100,750,1000,2000]  # I've removed a repeated 0.001 and 0.1 from your list\n",
    "\n",
    "#\n",
    "for feature in features_to_attack:\n",
    "    print(f\"\\nGenerating adversarial samples for {feature}...\\n\")\n",
    "    # List to store misclassification rates for each c value\n",
    "    misclassification_rates = []\n",
    "    unsuccessful_samples_indices_list=[]\n",
    "    adversarial_samples_list=[]\n",
    "    dt_miss_rate_list=[]\n",
    "\n",
    "    # Generate adversarial samples for each c value and store the misclassification rate\n",
    "    for iter in iteration_values:\n",
    "        print(\"Current iter:\",iter)\n",
    "        rate,dt_miss_rate,unsuccessful_samples_indices,adversarial_samples = generate_adversarial_for_c(0.01,iter,f_name=feature)\n",
    "        misclassification_rates.append(rate)\n",
    "        dt_miss_rate_list.append(dt_miss_rate)\n",
    "        unsuccessful_samples_indices_list.append(unsuccessful_samples_indices)\n",
    "        adversarial_samples_list.append(adversarial_samples)\n",
    "        print(f\"Processed for c value {0.0001}. Misclassification rate: {rate}\")\n",
    "    \n",
    "    with open(\"../output_iscx_new/\"+str(feature)+\"_data.pkl\", \"wb\") as file:\n",
    "        data = {\n",
    "            'adversarial_samples_list': adversarial_samples_list,\n",
    "            'misclassification_rates': misclassification_rates,\n",
    "            'dt_miss_rate_list': dt_miss_rate_list,\n",
    "            'unsuccessful_samples_indices_list': unsuccessful_samples_indices_list\n",
    "        }\n",
    "        pickle.dump(data, file)\n",
    "    dpi_value = 300 \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 7))  # Increase figure size for better clarity\n",
    "\n",
    "    plt.plot(iteration_values, misclassification_rates, '-o', label='surrogate Miss Rate')\n",
    "    plt.plot(iteration_values, dt_miss_rate_list, '-s', color='red', label='DT Miss Rate')  # Using different marker for clarity\n",
    "\n",
    "\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title('NN-MR(%) & DT-MR(%) vs. iteration')\n",
    "    plt.legend()  # To distinguish between the two lines on the plot\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", c='0.7')  # Adding grid for better readability\n",
    "    plt.savefig(f\"../output_iscx_new/\"+str(feature)+\"_NN_DT_MR_vs_iteration.png\",dpi=dpi_value)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size\n",
    "\n",
    "    l2_distances = []\n",
    "\n",
    "    for idx in range(len(iteration_values)):\n",
    "        unsuccessful_samples_indices = unsuccessful_samples_indices_list[idx]\n",
    "        malware_scale = np.delete(X_test_malware, unsuccessful_samples_indices, axis=0)\n",
    "        adversarial_samples_scale = adversarial_samples_list[idx]\n",
    "         # Check if the adversarial_samples_scale is empty\n",
    "        if len(adversarial_samples_scale) == 0:\n",
    "            print(f\"Skipping iteration {idx} due to empty adversarial samples.\")\n",
    "            l2_distances.append(float('nan'))  # append NaN or some placeholder value for the missing data point\n",
    "            continue\n",
    "        adversarial_samples_scale_arr = np.array(adversarial_samples_scale)\n",
    "        malware_scale=malware_scale.astype(np.float32)\n",
    "        adversarial_samples_ori_space = scaler.inverse_transform(adversarial_samples_scale_arr)\n",
    "        malware_samples_ori = scaler.inverse_transform(malware_scale)\n",
    "        \n",
    "        # Compute L2 distance for SrcBytes\n",
    "        # l2_distance = np.sqrt(np.sum((malware_samples_ori[:,7] - adversarial_samples_ori_space[:,7])**2))\n",
    "        # l2_distances.append(l2_distance)\n",
    "\n",
    "        l2_dist = np.linalg.norm(malware_samples_ori - adversarial_samples_ori_space, axis=1).mean()\n",
    "        l2_distances.append(l2_dist)\n",
    "\n",
    "    # Plotting\n",
    "\n",
    "    plt.figure(figsize=(10, 6))  # Increase figure size\n",
    "\n",
    "    # Primary y-axis settings (L2 Distance)\n",
    "    ax1 = plt.gca()\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('L2 Distance', color=color)\n",
    "    ax1.plot(iteration_values, l2_distances, color=color, linewidth=2, linestyle='--', marker='o')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, alpha=0.2)\n",
    "\n",
    "    # Secondary y-axis settings (Misclassification Rate)\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Misclassification Rate', color=color)\n",
    "    ax2.plot(iteration_values, misclassification_rates, color=color, linewidth=2, linestyle='-.', marker='x')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Adjust title and display\n",
    "    plt.title('Trade-off: Perturbation Magnitude vs Misclassification Rate', fontsize=16)\n",
    "    plt.tight_layout()  # Ensure no overlap of labels and title\n",
    "    plt.savefig(f\"../output_iscx/\"+str(feature)+\"_L2_vs_Misclassification_Rate.png\",dpi=dpi_value)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('srcBytes_adv.pkl','wb') as file:\n",
    "    pickle.dump(adversarial_samples_list,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
